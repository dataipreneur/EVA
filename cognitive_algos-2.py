# -*- coding: utf-8 -*-
"""Cognitive_algos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ipnQXkNTdlU23VSNrmKFxq_hD598bLa7
"""

import os
# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive")
#!ls

import json
import pandas as pd

#Loading JSON file
f7 = open('raw_response (12).json')
data = json.load(f7)
data.keys()

#Checking Data
#Based on categories
data_categories = data['categories']
data_categories
data_list = data['list']

#Converting categories into df
pd_list =pd.DataFrame(data_list)

pd_list['rate'][0]

#Distance with unit extraction
distWithUnit = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in pd_list['distanceWithUnit']],ignore_index=True)
distWithUnit.head()

#Image extraction
images = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in pd_list['images']],ignore_index=True)
images.head()

#Location extraction
location = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in pd_list['location']],ignore_index=True)
location.head()
print(location.columns)

#Vehicle Extraction
vehicle = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in pd_list['vehicle']],ignore_index=True)
vehicle.head()

#Rate extraction
rate = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in pd_list['rate']],ignore_index=True)

#Getting relevant data
rate_df = rate[['averageDailyPrice','monthly','weekly']]
df=pd_list[['distance','rating','renterTripsTaken','reviewCount','responseRate','newListing','fuelType','responseTime']]
dist_with_unit_df =distWithUnit[['scalar','unit']]
Images_df=images[['originalImageUrl','thumbnails.170x102']]
location_df=location[['addressLines', 'city', 'country', 'latitude', 'longitude', 'state', 'timeZone']]
vehicle_df=vehicle[['model','make','id','listingCreatedTime','year','type','automaticTransmission']]

#Combining all relevant columns - DF -new_df
pdList = [df, dist_with_unit_df, Images_df, location_df, vehicle_df, rate_df]  # List of your dataframes
new_df = pd.concat(pdList,axis=1)

#Creating new columns for convenience
new_df['Model'] = new_df['model'].apply(lambda x : x.upper())
new_df['Make'] = new_df['make'].apply(lambda x : x.upper())

new_df

new_df[['Model', 'Make']]

df_cars = pd.read_csv('Electric_Vehicle_Population_Data-2.csv')



car_make_model = new_df[['model', 'make']]
car_make_model.rename(columns={'model': 'Model', 'make': 'Make'}, inplace=True)

#Creating a lookup table with average values
grouped_attributes = new_df.groupby(['Make','Model'])
grp_atri_df = grouped_attributes.first()

#Printing group attributes
grp_atri_df.columns

#Fetching relevant columns for the table
lookup_table = grp_atri_df[['rating','renterTripsTaken','reviewCount','type']]

#Merging with main table for mapping
main = pd.merge(df_cars, lookup_table, on =['Make','Model'],how ='inner')

#Location processing
print((main['Vehicle Location'][1]))

print((main['Vehicle Location'][1].split(' ',1))[1].split())

#Applying regex for number extarction
import re

#s = "POINT (-122.31768 47.87166)"
#numbers = re.findall(r"[-+]?\d*\.\d+|\d+", s)

main = main.dropna()

main = main.reset_index()
main['Vehicle Location'] = main['Vehicle Location'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x))


#print(numbers)

main['State'].unique()

# Creating two separate columns for lat and long
main

main['lat'] = main['Vehicle Location'].apply(lambda x: float(x[0]))
main['lng'] = main['Vehicle Location'].apply(lambda x: float(x[1]))

main

import random

random_list = []
# Set a length of the list to 10
for i in range(0, len(main)):
    # any random numbers from 0 to 1000
    random_list.append(random.randint(30000, 70000))
print(random_list)

main['Base MSRP'] = random_list

main['Price per day'] = 0.003*main['Base MSRP']


len(main)

main_csv_data = main.to_csv('ev_main2.csv', index = True)

print(main.columns)
main

#Picking up some relevant fields
final = main[['County', 'City', 'State', 'Postal Code', 'Model Year',
       'Make', 'Model', 'Electric Vehicle Type', 'Price per day',
       'Clean Alternative Fuel Vehicle (CAFV) Eligibility', 'Electric Range','rating',
       'renterTripsTaken', 'reviewCount', 'type', 'lat', 'lng']]
final = final.dropna()
final1000 = final[0:1000]
print(type(final['lat'][0]))

#Normalizing the continous data
from sklearn import preprocessing
final_norm = final.copy()
scaler = preprocessing.MinMaxScaler()
final_norm[['Postal Code', 'Model Year','Electric Range','rating', 'Price per day',
       'renterTripsTaken', 'reviewCount', 'lat', 'lng']]= scaler.fit_transform(final_norm[['Postal Code', 'Model Year','Electric Range','rating', 'Price per day',
       'renterTripsTaken', 'reviewCount', 'lat', 'lng']])

final_norm[['Postal Code', 'Model Year','Electric Range','rating',
       'renterTripsTaken', 'reviewCount', 'lat', 'lng']]

#final_norm1000 = final_norm[0:1000]
final_norm.columns

!pip install kmodes
from kmodes.kprototypes import KPrototypes
kproto = KPrototypes(n_clusters=5, init='Cao')
clusters = kproto.fit_predict(final_norm, categorical=[0, 1, 2, 3, 5, 6, 7, 9, 14])
##join data with labels
labels = pd.DataFrame(clusters)
labeledCustomers = pd.concat((final,labels),axis=1)
labeledCustomers = labeledCustomers.rename({0:'labels'},axis=1)

len(labels)

len(final1000)



##Collecting the labelled data as per clusters
labeledCustomers = labeledCustomers.dropna()
labeledCustomers = labeledCustomers.reset_index()

print(type(labeledCustomers['lat'][0]))
print(labeledCustomers)

#OPTIONAL: Elbow plot with cost (will take a LONG time)
costs = []
n_clusters = []
clusters_assigned = []

for i in range(2, 25):
    try:
        kproto = KPrototypes(n_clusters= i, init='Cao', verbose=1)
        clusters = kproto.fit_predict(final_norm, categorical=[0, 1, 2, 3, 5, 6, 7, 9, 14])
        costs.append(kproto.cost_)
        n_clusters.append(i)
        clusters_assigned.append(clusters)
    except:
        print(f"Can't cluster with {i} clusters")

fig = go.Figure(data=go.Scatter(x=n_clusters, y=costs ))
fig.show()

import plotly.express as px

# Example latitudes and longitudes

df = labeledCustomers.dropna()


fig = px.scatter_mapbox(df, lat="lat", lon="lng", color_continuous_scale=px.colors.cyclical.IceFire, size_max=15, zoom=10)

fig.update_layout(
    hovermode='closest',
    mapbox=dict(pitch=0,
        zoom=10
    )
)
fig.show()

stations = pd.read_csv('ev_stations_v1.csv')
stations

print(stations.columns)

stations_loc = stations[['Latitude','Longitude']]
stations_loc = stations_loc.dropna()

##Locations based on clustering using KMEANS
from sklearn.cluster import KMeans


#OPTIONAL: Elbow plot with inertia
#Elbow method to choose the optimal number of clusters
sse = {}
for k in range(2, 50):
    kmeans = KMeans(n_clusters=k, max_iter=100).fit(stations_loc)
    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center

fig = go.Figure(data=go.Scatter(x=list(sse.keys()), y=list(sse.values())))
fig.show()

